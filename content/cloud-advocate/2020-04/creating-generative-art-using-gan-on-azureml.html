<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Creating Generative Art using GANs on Azure ML | Azure Selected</title>
    <meta name="generator" content="VuePress 1.9.8">
    <link rel="icon" href="/azureselected/favicon.ico">
    <meta name="description" content="How you can train GANs on pictures of flowers and portraits. This technical post should be first, because the AI Art challenge would depend on it.">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    
    <link rel="preload" href="/azureselected/assets/css/0.styles.fe1e9048.css" as="style"><link rel="preload" href="/azureselected/assets/js/app.8a0b7483.js" as="script"><link rel="preload" href="/azureselected/assets/js/2.8d021b4d.js" as="script"><link rel="preload" href="/azureselected/assets/js/31.6b43ffde.js" as="script"><link rel="preload" href="/azureselected/assets/js/3.32c85e2e.js" as="script"><link rel="preload" href="/azureselected/assets/js/5.554b9b17.js" as="script"><link rel="prefetch" href="/azureselected/assets/js/10.fa73f6fc.js"><link rel="prefetch" href="/azureselected/assets/js/11.e07babbc.js"><link rel="prefetch" href="/azureselected/assets/js/12.12214fd4.js"><link rel="prefetch" href="/azureselected/assets/js/13.b7275e3f.js"><link rel="prefetch" href="/azureselected/assets/js/14.d6b8a728.js"><link rel="prefetch" href="/azureselected/assets/js/15.31a19f47.js"><link rel="prefetch" href="/azureselected/assets/js/16.c3ccb867.js"><link rel="prefetch" href="/azureselected/assets/js/17.7d290d04.js"><link rel="prefetch" href="/azureselected/assets/js/18.4cc5d13c.js"><link rel="prefetch" href="/azureselected/assets/js/19.01296b5f.js"><link rel="prefetch" href="/azureselected/assets/js/20.84db9ad2.js"><link rel="prefetch" href="/azureselected/assets/js/21.6a1d22b6.js"><link rel="prefetch" href="/azureselected/assets/js/22.b38195a7.js"><link rel="prefetch" href="/azureselected/assets/js/23.53dea98b.js"><link rel="prefetch" href="/azureselected/assets/js/24.67816702.js"><link rel="prefetch" href="/azureselected/assets/js/25.8e0307d1.js"><link rel="prefetch" href="/azureselected/assets/js/26.fb128a8f.js"><link rel="prefetch" href="/azureselected/assets/js/27.e8b0b7a9.js"><link rel="prefetch" href="/azureselected/assets/js/28.e625e9de.js"><link rel="prefetch" href="/azureselected/assets/js/29.aeb087ba.js"><link rel="prefetch" href="/azureselected/assets/js/30.8ef68534.js"><link rel="prefetch" href="/azureselected/assets/js/32.b2f60498.js"><link rel="prefetch" href="/azureselected/assets/js/33.ef0db925.js"><link rel="prefetch" href="/azureselected/assets/js/34.c990c7e7.js"><link rel="prefetch" href="/azureselected/assets/js/35.177548dd.js"><link rel="prefetch" href="/azureselected/assets/js/36.be924c7e.js"><link rel="prefetch" href="/azureselected/assets/js/37.c4a329a3.js"><link rel="prefetch" href="/azureselected/assets/js/38.88502f70.js"><link rel="prefetch" href="/azureselected/assets/js/39.b70a599f.js"><link rel="prefetch" href="/azureselected/assets/js/4.f7cbda98.js"><link rel="prefetch" href="/azureselected/assets/js/40.af1f7c88.js"><link rel="prefetch" href="/azureselected/assets/js/41.09f3637f.js"><link rel="prefetch" href="/azureselected/assets/js/42.91268a34.js"><link rel="prefetch" href="/azureselected/assets/js/43.8bc4e597.js"><link rel="prefetch" href="/azureselected/assets/js/44.e75fe39d.js"><link rel="prefetch" href="/azureselected/assets/js/45.36f7e780.js"><link rel="prefetch" href="/azureselected/assets/js/46.d83d96f0.js"><link rel="prefetch" href="/azureselected/assets/js/47.b54cc2ca.js"><link rel="prefetch" href="/azureselected/assets/js/48.9b7655b1.js"><link rel="prefetch" href="/azureselected/assets/js/49.dc3e7585.js"><link rel="prefetch" href="/azureselected/assets/js/50.44491727.js"><link rel="prefetch" href="/azureselected/assets/js/51.b3cc832a.js"><link rel="prefetch" href="/azureselected/assets/js/52.4b3f7433.js"><link rel="prefetch" href="/azureselected/assets/js/53.d0efb5a8.js"><link rel="prefetch" href="/azureselected/assets/js/54.41189e23.js"><link rel="prefetch" href="/azureselected/assets/js/55.0604b61d.js"><link rel="prefetch" href="/azureselected/assets/js/56.d1bb56be.js"><link rel="prefetch" href="/azureselected/assets/js/57.2ae826e1.js"><link rel="prefetch" href="/azureselected/assets/js/58.fcca3b58.js"><link rel="prefetch" href="/azureselected/assets/js/59.3da7d882.js"><link rel="prefetch" href="/azureselected/assets/js/6.ef320c88.js"><link rel="prefetch" href="/azureselected/assets/js/60.3f06229c.js"><link rel="prefetch" href="/azureselected/assets/js/61.6e57367f.js"><link rel="prefetch" href="/azureselected/assets/js/62.3b222c6c.js"><link rel="prefetch" href="/azureselected/assets/js/63.8a7f0932.js"><link rel="prefetch" href="/azureselected/assets/js/7.51b986da.js"><link rel="prefetch" href="/azureselected/assets/js/8.a9358b51.js"><link rel="prefetch" href="/azureselected/assets/js/9.b8eed478.js">
    <link rel="stylesheet" href="/azureselected/assets/css/0.styles.fe1e9048.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/azureselected/" class="home-link router-link-active"><img src="/azureselected/img/logo_azure.svg" alt="Azure Selected" class="logo"> <span class="site-name can-hide">Azure Selected</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/azureselected/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/azureselected/content/" class="nav-link router-link-active">
  Content
</a></div><div class="nav-item"><a href="/azureselected/tags.html" class="nav-link">
  Tags
</a></div><div class="nav-item"><a href="https://wj.qq.com/s2/5227985/7213/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Join Us
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Select language" class="dropdown-title"><span class="title">Language</span> <span class="arrow down"></span></button> <button type="button" aria-label="Select language" class="mobile-dropdown-title"><span class="title">Language</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/azureselected/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html" aria-current="page" class="nav-link router-link-exact-active router-link-active">
  English
</a></li><li class="dropdown-item"><!----> <a href="/azureselected/zh-cn/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html" class="nav-link">
  简体中文
</a></li><li class="dropdown-item"><!----> <a href="/azureselected/zh-tw/" class="nav-link">
  繁體中文
</a></li></ul></div></div> <a href="https://github.com/shinyzhu/azureselected" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/azureselected/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/azureselected/content/" class="nav-link router-link-active">
  Content
</a></div><div class="nav-item"><a href="/azureselected/tags.html" class="nav-link">
  Tags
</a></div><div class="nav-item"><a href="https://wj.qq.com/s2/5227985/7213/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Join Us
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Select language" class="dropdown-title"><span class="title">Language</span> <span class="arrow down"></span></button> <button type="button" aria-label="Select language" class="mobile-dropdown-title"><span class="title">Language</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/azureselected/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html" aria-current="page" class="nav-link router-link-exact-active router-link-active">
  English
</a></li><li class="dropdown-item"><!----> <a href="/azureselected/zh-cn/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html" class="nav-link">
  简体中文
</a></li><li class="dropdown-item"><!----> <a href="/azureselected/zh-tw/" class="nav-link">
  繁體中文
</a></li></ul></div></div> <a href="https://github.com/shinyzhu/azureselected" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Creating Generative Art using GANs on Azure ML</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/azureselected/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html#generative-adversarial-networks" class="sidebar-link">Generative Adversarial Networks</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/azureselected/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html#discriminator-model" class="sidebar-link">Discriminator Model</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/azureselected/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html#generator-model" class="sidebar-link">Generator Model</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/azureselected/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html#training-script-for-azure-ml" class="sidebar-link">Training script for Azure ML</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/azureselected/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html#starting-the-experiment" class="sidebar-link">Starting the Experiment</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/azureselected/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html#running-many-experiments" class="sidebar-link">Running Many Experiments</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/azureselected/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html#getting-experiment-results" class="sidebar-link">Getting Experiment Results</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/azureselected/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html#generating-new-images" class="sidebar-link">Generating new Images</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/azureselected/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html#observing-the-process-of-learning" class="sidebar-link">Observing The Process of Learning</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/azureselected/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html#food-for-thought" class="sidebar-link">Food for Thought</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/azureselected/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html#acknowledgements" class="sidebar-link">Acknowledgements</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/azureselected/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.html#other-posts-in-azure-ml-series" class="sidebar-link">Other Posts in Azure ML Series</a><ul class="sidebar-sub-headers"></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="creating-generative-art-using-gans-on-azure-ml"><a href="#creating-generative-art-using-gans-on-azure-ml" class="header-anchor">#</a> Creating Generative Art using GANs on Azure ML</h1> <div class="content-meta"><hr> <p>From:  <a href="https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/">https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/</a></p> <p>By:  Dmitry Soshnikov | 3/6/2020</p> <!----> <div class="tags-head"><span><a href="/azureselected/tags.html#Azure Machine Learning">
      Azure Machine Learning
    </a></span></div> <hr></div> <p>Deep Learning can look like Magic! I get the most magical feeling when watching neural network doing something creative, for example learning to produce paintings like an artist. Technology behind this is called Generative Adversarial Networks, and in this post we will look at how to train such a network on Azure Machine Learning Service.</p> <blockquote><p>This post is a part of <a href="http://aka.ms/AIApril" target="_blank" rel="noopener noreferrer">AI April<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> initiative, where each day of April my colleagues publish new original article related to AI, Machine Learning and Microsoft. Have a look at the <a href="http://aka.ms/AIApril" target="_blank" rel="noopener noreferrer">Calendar<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> to find other interesting articles that have already been published, and keep checking that page during the month.</p></blockquote> <p>If you have seen my previous posts on Azure ML (about <a href="https://soshnikov.com/azure/best-way-to-start-with-azureml/" target="_blank" rel="noopener noreferrer">using it from VS Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> and <a href="https://soshnikov.com/azure/using-azureml-for-hyperparameter-optimization/" target="_blank" rel="noopener noreferrer">submitting experiments and hyperparameter optimization<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>), you should know that it is quite convenient to use Azure ML for almost any training tasks. However, all examples up to now have been done using toy MNIST dataset. Today we will focus on real problem: creating artificial paintings like those:</p> <table><thead><tr><th><img src="https://soshnikov.com/images/artartificial/Flo1.jpg" alt="Flowers"></th> <th><img src="https://soshnikov.com/images/artartificial/Port1.jpg" alt="Portrait"></th></tr></thead> <tbody><tr><td>Flowers, 2019, <em>Art of the Artificial</em> <a href="https://github.com/shwars/keragan" target="_blank" rel="noopener noreferrer">keragan<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> trained on <a href="https://www.wikiart.org/" target="_blank" rel="noopener noreferrer">WikiArt<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> Flowers</td> <td>Queen of Chaos, 2019, <a href="https://github.com/shwars/keragan" target="_blank" rel="noopener noreferrer">keragan<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> trained on <a href="https://www.wikiart.org/" target="_blank" rel="noopener noreferrer">WikiArt<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> Portraits</td></tr></tbody></table> <p>Those painting are produced after training the network on paintings from <a href="https://www.wikiart.org/" target="_blank" rel="noopener noreferrer">WikiArt<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>. If you want to reproduce the same results, you may need to collect the dataset yourself, for example by using <a href="https://github.com/lucasdavid/wikiart" target="_blank" rel="noopener noreferrer">WikiArt Retriever<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, or borrowing existing collections from [WikiArt Dataset](https://github.com/cs-chan/ArtGAN/blob/master/WikiArt Dataset/README.md) or <a href="https://github.com/rkjones4/GANGogh" target="_blank" rel="noopener noreferrer">GANGogh Project<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p>Place images you want to train on somewhere in <code>dataset</code> directory. For training on flowers, here is how some of those images might look like:</p> <p><img src="https://soshnikov.com/images/blog/gan_dataset_flowers.png" alt="Flowers Dataset"></p> <p>We need our neural network model to learn both high-level composition of flower bouquet and a vase, as well as low-level style of painting, with smears of paint and canvas texture.</p> <h2 id="generative-adversarial-networks"><a href="#generative-adversarial-networks" class="header-anchor">#</a> Generative Adversarial Networks</h2> <p>Those painting were generated using <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" target="_blank" rel="noopener noreferrer"><strong>Generative Adversarial Network</strong><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>), or GAN for short. In this example, we will use my simple GAN implementation in Keras called <a href="https://github.com/shwars/keragan" target="_blank" rel="noopener noreferrer">keragan<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, and I will show some simplified code parts from it.</p> <p>GAN consists of two networks:</p> <ul><li><strong>Generator</strong>, which generates images given some input <strong>noise vector</strong></li> <li><strong>Discriminator</strong>, whose role is to differentiate between real and “fake” (generated) paintings</li></ul> <p><img src="https://soshnikov.com/images/blog/gan_architecture.png" alt="GAN Architecture"></p> <p>Training the GAN involves the following steps:</p> <ol><li><p>Getting a bunch of generated and real images:</p> <div class="language- extra-class"><pre class="language-text"><code>noise = np.random.normal(0, 1, (batch_size, latent_dim))
gen_imgs = generator.predict(noise)   
imgs = get_batch(batch_size)
</code></pre></div></li> <li><p>Training discriminator to better differentiate between those two. Note, how we provide vector with</p> <div class="language-plaintext extra-class"><pre class="language-plaintext"><code>ones
</code></pre></div><p>and</p> <div class="language-plaintext extra-class"><pre class="language-plaintext"><code>zeros
</code></pre></div><p>as expected answer:</p> <div class="language- extra-class"><pre class="language-text"><code>d_loss_r = discriminator.train_on_batch(imgs, ones)
d_loss_f = discriminator.train_on_batch(gen_imgs, zeros)
d_loss = np.add(d_loss_r , d_loss_f)*0.5
</code></pre></div></li> <li><p>Training the combined model, in order to improve the generator:</p> <div class="language- extra-class"><pre class="language-text"><code>g_loss = combined.train_on_batch(noise, ones)
</code></pre></div></li></ol> <p>During this step, discriminator is not trained, because its weights are explicitly frozen during creation of combined model:</p> <div class="language- extra-class"><pre class="language-text"><code>discriminator = create_discriminator()
generator = create_generator()
discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, 
                      metrics=['accuracy'])
discriminator.trainable = False

z = keras.models.Input(shape=(latent_dim,))
img = generator(z)
valid = discriminator(img)

combined = keras.models.Model(z, valid) 
combined.compile(loss='binary_crossentropy', optimizer=optimizer)
</code></pre></div><h2 id="discriminator-model"><a href="#discriminator-model" class="header-anchor">#</a> Discriminator Model</h2> <p>To differentiate between real and fake image, we use traditional <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" target="_blank" rel="noopener noreferrer"><strong>Convolutional Neural Network</strong><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> (CNN) architecture. So, for the image of size 64x64, we will have something like this:</p> <div class="language- extra-class"><pre class="language-text"><code>discriminator = Sequential()

for x in [16,32,64]: # number of filters on next layer
    discriminator.add(Conv2D(x, (3,3), strides=1, padding=&quot;same&quot;))
    discriminator.add(AveragePooling2D())
    discriminator.addBatchNormalization(momentum=0.8))
    discriminator.add(LeakyReLU(alpha=0.2))
    discriminator.add(Dropout(0.3))

discriminator.add(Flatten())
discriminator.add(Dense(1, activation='sigmoid'))
</code></pre></div><p>We have 3 convolution layers, which do the following:</p> <ul><li>Original image of shape 64x64x3 is passed over by 16 filters, resulting in a shape 32x32x16. To decrease the size, we use <code>AveragePooling2D</code>.</li> <li>Next step converts 32x32x16 tensor into 16x16x32</li> <li>Finally, after the next convolution layer, we end up with tensor of shape 8x8x64.</li></ul> <p>On top of this convolutional base, we put simple logistic regression classifier (AKA 1-neuron dense layer).</p> <h2 id="generator-model"><a href="#generator-model" class="header-anchor">#</a> Generator Model</h2> <p>Generator model is slightly more complicated. First, imagine if we wanted to convert an image to some sort of feature vector of length <code>latent_dim=100</code>. We would use convolutional network model similar to the discriminator above, but final layer would be a dense layer with size 100.</p> <p>Generator does the opposite — converts vector of size 100 to an image. This involves a process called <strong>deconvolution</strong>, which is essentially a <em>reversed convolution</em>. Together with <code>UpSampling2D</code> they cause the size of the tensor to increase at each layer:</p> <div class="language- extra-class"><pre class="language-text"><code>generator = Sequential()
generator.add(Dense(8 * 8 * 2 * size, activation=&quot;relu&quot;, 
                                      input_dim=latent_dim))
generator.add(Reshape((8, 8, 2 * size)))

for x in [64;32;16]:
    generator.add(UpSampling2D())
    generator.add(Conv2D(x, kernel_size=(3,3),strides=1,padding=&quot;same&quot;))
    generator.add(BatchNormalization(momentum=0.8))
    generator.add(Activation(&quot;relu&quot;))

generator.add(Conv2D(3, kernel_size=3, padding=&quot;same&quot;))
generator.add(Activation(&quot;tanh&quot;))
</code></pre></div><p>At the last step, we end up with tensor size 64x64x3, which is exactly the size of the image that we need. Note that final activation function is <code>tanh</code>, which gives an output in the range of [-1;1] — which means that we need to scale original training images to this interval. All those steps for preparing images is handled by <code>ImageDataset</code> class, and I will not go into detail there.</p> <h2 id="training-script-for-azure-ml"><a href="#training-script-for-azure-ml" class="header-anchor">#</a> Training script for Azure ML</h2> <p>Now that we have all pieces for training the GAN together, we are ready to run this code on Azure ML as an experiment!</p> <p>There is one important thing to be noted, however: normally, when running an experiment in Azure ML, we want to track some metrics, such as accuracy or loss. We can log those values during training using <code>run.log</code>, as described in my <a href="https://soshnikov.com/azure/best-way-to-start-with-azureml/" target="_blank" rel="noopener noreferrer">previous post<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, and see how this metric changes during training on <a href="http://ml.azure.com/?WT.mc_id=aiapril-blog-dmitryso" target="_blank" rel="noopener noreferrer">Azure ML Portal<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p>In our case, instead of numeric metric, we are interested in the visual images that our network generates at each step. Inspecting those images while experiment is running can help us decide whether we want to end our experiment, alter parameters, or continue.</p> <p>Azure ML supports logging images in addition to numbers, as described <a href="https://docs.microsoft.com/azure/machine-learning/how-to-track-experiments/?WT.mc_id=aiapril-blog-dmitryso" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>. We can log either images represented as np-arrays, or any plots produced by <code>matplotlib</code>, so what we will do is plotting three sample images on one plot. This plotting will be handled in <code>callbk</code> callback function that gets called by <code>keragan</code> after each training epoch:</p> <div class="language- extra-class"><pre class="language-text"><code>def callbk(tr):
    if tr.gan.epoch % 20 == 0:
        res = tr.gan.sample_images(n=3)
        fig,ax = plt.subplots(1,len(res))
        for i,v in enumerate(res):
            ax[i].imshow(v[0])
        run.log_image(&quot;Sample&quot;,plot=plt)
</code></pre></div><p>So, the actual training code will look like this:</p> <div class="language- extra-class"><pre class="language-text"><code>gan = keragan.DCGAN(args)
imsrc = keragan.ImageDataset(args)
imsrc.load()
train = keragan.GANTrainer(image_dataset=imsrc,gan=gan,args=args)

train.train(callbk)
</code></pre></div><p>Note that <code>keragan</code> supports automatic parsing of many command-line parameters that we can pass to it through <code>args</code> structure, and that is what makes this code so simple.</p> <h2 id="starting-the-experiment"><a href="#starting-the-experiment" class="header-anchor">#</a> Starting the Experiment</h2> <p>To submit the experiment to Azure ML, we will use the code similar to the one discussed in the <a href="https://soshnikov.com/azure/using-azureml-for-hyperparameter-optimization/" target="_blank" rel="noopener noreferrer">previous post on Azure ML<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>. The code is located inside [submit_gan.ipynb][https://github.com/CloudAdvocacy/AzureMLStarter/blob/master/submit_gan.ipynb], and it starts with familiar steps:</p> <ul><li>Connecting to the Workspace using <code>ws = Workspace.from_config()</code></li> <li>Connecting to the Compute cluster: <code>cluster = ComputeTarget(workspace=ws, name='My Cluster')</code>. Here we need a cluster of GPU-enabled VMs, such as <a href="https://docs.microsoft.com/azure/virtual-machines/sizes-gpu/?WT.mc_id=aiapril-blog-dmitryso" target="_blank" rel="noopener noreferrer">NC6<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</li> <li>Uploading our dataset to the default datastore in the ML Workspace</li></ul> <p>After that has been done, we can submit the experiment using the following code:</p> <div class="language- extra-class"><pre class="language-text"><code>exp = Experiment(workspace=ws, name='KeraGAN')
script_params = {
    '--path': ws.get_default_datastore(),
    '--dataset' : 'faces',
    '--model_path' : './outputs/models',
    '--samples_path' : './outputs/samples',
    '--batch_size' : 32,
    '--size' : 512,
    '--learning_rate': 0.0001,
    '--epochs' : 10000
}
est = TensorFlow(source_directory='.',
    script_params=script_params,
    compute_target=cluster,
    entry_script='train_gan.py',
    use_gpu = True,
    conda_packages=['keras','tensorflow','opencv','tqdm','matplotlib'],
    pip_packages=['git+https://github.com/shwars/keragan@v0.0.1']

run = exp.submit(est)
</code></pre></div><p>In our case, we pass <code>model_path=./outputs/models</code> and <code>samples_path=./outputs/samples</code> as parameters, which will cause models and samples generated during training to be written to corresponding directories inside Azure ML experiment. Those files will be available through Azure ML Portal, and can also be downloaded programmatically afterwards (or even during training).</p> <p>To create the estimator that can run on GPU without problems, we use built-in <a href="https://docs.microsoft.com/python/api/azureml-train-core/azureml.train.dnn.tensorflow?view=azure-ml-py&amp;WT.mc_id=aiapril-blog-dmitryso" target="_blank" rel="noopener noreferrer"><code>Tensorflow</code><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> estimator. It is very similar to generic <a href="https://docs.microsoft.com/python/api/azureml-train-core/azureml.train.estimator.estimator?view=azure-ml-py&amp;WT.mc_id=aiapril-blog-dmitryso" target="_blank" rel="noopener noreferrer"><code>Estimator</code><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, but also provides some out-of-the-box options for distributed training. You can read more about using different estimators <a href="https://docs.microsoft.com/azure/machine-learning/how-to-train-ml-models?WT.mc_id=aiapril-blog-dmitryso" target="_blank" rel="noopener noreferrer">in the official documentation<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p>Another interesting point here is how we install <code>keragan</code> library — directly from GitHub. While we can also install it from PyPI repository, I wanted to demonstrate you that direct installation from GitHub is also supported, and you can even indicate a specific version of the library, tag or commit ID.</p> <p>After the experiment has been running for some time, we should be able to observe the sample images being generated in the <a href="http://ml.azure.com/?WT.mc_id=aiapril-blog-dmitryso" target="_blank" rel="noopener noreferrer">Azure ML Portal<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>:</p> <p><img src="https://soshnikov.com/images/blog/AzML/AzMLPortalGAN.PNG" alt="GAN Training Experiment Results"></p> <h2 id="running-many-experiments"><a href="#running-many-experiments" class="header-anchor">#</a> Running Many Experiments</h2> <p>The first time we run GAN training, we might not get excellent results, for several reasons. First of all, learning rate seems to be an important parameter, and too high learning rate might lead to poor results. Thus, for best results we might need to perform a number of experiments.</p> <p>Parameters that we might want to vary are the following:</p> <ul><li><code>--size</code> determines the size of the picture, which should be power of 2. Small sizes like 64 or 128 allow for fast exprimentation, while large sizes (up to 1024) are good for creating higher quality images. Anything above 1024 will likely not produce good results, because special techniques are required to train large resolutions GANs, such as <a href="https://arxiv.org/abs/1710.10196" target="_blank" rel="noopener noreferrer">progressive growing<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><code>--learning_rate</code> is surprisingly quite an important parameter, especially with higher resolutions. Smaller learning rate typically gives better results, but training happens very slowly.</li> <li><code>--dateset</code>. We might want to upload pictures of different styles into different folders in the Azure ML datastore, and start training multiple experiments simultaneously.</li></ul> <p>Since we already know how to submit the experiment programmatically, it should be easy to wrap that code into a couple of <code>for</code>-loops to perform some parametric sweep. You may then check manually through Azure ML Portal which experiments are on their way to good results, and terminate all other experiments to save costs. Having a cluster of a few VMs gives you the freedom to start a few experiments at the same time without waiting.</p> <h2 id="getting-experiment-results"><a href="#getting-experiment-results" class="header-anchor">#</a> Getting Experiment Results</h2> <p>Once you are happy with results, it makes sense to get the results of the training in the form or model files and sample images. I have mentioned that during the training our training script stored models in <code>outputs/models</code> directory, and sample images — to <code>outputs/samples</code>. You can browse those directories in the Azure ML Portal and download the files that you like manually:</p> <p><img src="https://soshnikov.com/images/blog/AzML/AzMLPortalGANRes.PNG" alt="Azure Portal with Experiment Results"></p> <p>You can also do that programmatically, especially if you want to download <em>all</em> images produced during different epochs. <code>run</code> object that you have obtained during experiment submission gives you access to all files stored as part of that run, and you can download them like this:</p> <div class="language- extra-class"><pre class="language-text"><code>run.download_files(prefix='outputs/samples')
</code></pre></div><p>This will create the directory <code>outputs/samples</code> inside the current directory, and download all files from remote directory with the same name.</p> <p>I you have lost the reference to the specific run inside your notebook (it can happen, because most of the experiments are quite long-running), you can always create it by knowing the <em>run id</em>, which you can look up at the portal:</p> <div class="language- extra-class"><pre class="language-text"><code>run = Run(experiment=exp,run_id='KeraGAN_1584082108_356cf603')
</code></pre></div><p>We can also get the models that were trained. For example, let’s download the final generator model, and use it for generating a bunch of random images. We can get all filenames that are associated with the experiment, and filter out only those that represent generator models:</p> <div class="language- extra-class"><pre class="language-text"><code>fnames = run.get_file_names()
fnames = filter(lambda x : x.startswith('outputs/models/gen_'),fnames)
</code></pre></div><p>They will all look like <code>outputs/models/gen_0.h5</code>, <code>outputs/models/gen_100.h5</code> and so on. We need to find out the maximum epoch number:</p> <div class="language- extra-class"><pre class="language-text"><code>no = max(map(lambda x: int(x[19:x.find('.')]), fnames))
fname = 'outputs/models/gen_{}.h5'.format(no)
fname_wout_path = fname[fname.rfind('/')+1:]
run.download_file(fname)
</code></pre></div><p>This will download the file with the highers epoch number to local directory, and also store the name of this file (w/out directory path) into <code>fname_wout_path</code>.</p> <h2 id="generating-new-images"><a href="#generating-new-images" class="header-anchor">#</a> Generating new Images</h2> <p>Once we have obtained the model, we can just need load it in Keras, find out the input size, and give the correctly sized random vector as the input to produce new random painting generated by the network:</p> <div class="language- extra-class"><pre class="language-text"><code>model = keras.models.load_model(fname_wout_path)
latent_dim=model.layers[0].input.shape[1].value
res = model.predict(np.random.normal(0,1,(10,latent_dim)))
</code></pre></div><p>Output of the generator network is in the range [-1,1], so we need to scale it linearly to the range [0,1] in order to be correctly displayed by <code>matplotlib</code>:</p> <div class="language- extra-class"><pre class="language-text"><code>res = (res+1.0)/2
fig,ax = plt.subplots(1,10,figsize=(15,10))
for i in range(10):
    ax[i].imshow(res[i])
</code></pre></div><p>Here is the result we will get: <img src="https://soshnikov.com/images/blog/AzML/AzMLGANPix.PNG" alt="GAN Result"></p> <p>Have a look at some of the best pictures produced during this experiment:</p> <table><thead><tr><th style="text-align:left;"><img src="https://soshnikov.com/images/artartificial/ColorfulSpring.jpg" alt="Colourful Spring"></th> <th style="text-align:left;"><img src="https://soshnikov.com/images/artartificial/CountrySide.jpg" alt="Countryside"></th></tr></thead> <tbody><tr><td style="text-align:left;"><em>Colourful Spring</em>, 2020</td> <td style="text-align:left;"><em>Countryside</em>, 2020</td></tr> <tr><td style="text-align:left;"><a href="https://github.com/shwars/keragan" target="_blank" rel="noopener noreferrer">keragan<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> trained on <a href="https://www.wikiart.org/" target="_blank" rel="noopener noreferrer">WikiArt<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>Impressionism</td> <td style="text-align:left;"></td></tr> <tr><td style="text-align:left;"><img src="https://soshnikov.com/images/artartificial/ThruIcyGlass.jpg" alt="Summer Landscape"></td> <td style="text-align:left;"><img src="https://soshnikov.com/images/artartificial/SummerLandscape.jpg" alt="Summer Landscape"></td></tr> <tr><td style="text-align:left;"><em>Through the Icy Glass</em>, 2020</td> <td style="text-align:left;"><em>Summer Landscape</em>, 2020</td></tr> <tr><td style="text-align:left;"><a href="https://github.com/shwars/keragan" target="_blank" rel="noopener noreferrer">keragan<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> trained on <a href="https://www.wikiart.org/" target="_blank" rel="noopener noreferrer">WikiArt<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> Impressionism</td> <td style="text-align:left;"></td></tr></tbody></table> <blockquote><p>If you want to get fresh images produced by the network every day (well, almost every day) - we (together with my daughter) have created <a href="http://instagram.com/art_of_artificial" target="_blank" rel="noopener noreferrer">@art_of_artificial<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> Instagram account where we will post those images.</p></blockquote> <h2 id="observing-the-process-of-learning"><a href="#observing-the-process-of-learning" class="header-anchor">#</a> Observing The Process of Learning</h2> <p>It is also interesting to look at the process of how GAN network gradually learns. I have explored this notion of learning in my exhibition <a href="https://soshnikov.com/art/artofartificial" target="_blank" rel="noopener noreferrer">Art of the Artificial<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>. Here are a couple of videos that show this process:</p> <table><thead><tr><th>GAN Flower Generation</th> <th>GAN Portrait Generation</th></tr></thead> <tbody><tr><td><a href="https://youtu.be/hnwbnt2Q9Iw" target="_blank" rel="noopener noreferrer">https://youtu.be/hnwbnt2Q9Iw<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></td> <td><a href="https://youtu.be/j2wpUFxyrEs" target="_blank" rel="noopener noreferrer">https://youtu.be/j2wpUFxyrEs<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></td></tr></tbody></table> <h2 id="food-for-thought"><a href="#food-for-thought" class="header-anchor">#</a> Food for Thought</h2> <p>In this post, I have described how GAN works, and how to train it using Azure ML. This definitely opens up a lot of room for experimentation, but also a lot of room for thought. During this experiment we have created original artworks, generated by Artificial Intelligence. But can they be considered <strong>ART</strong>? I will discuss this in one of my next posts…</p> <h2 id="acknowledgements"><a href="#acknowledgements" class="header-anchor">#</a> Acknowledgements</h2> <p>When producing <a href="https://github.com/shwars/keragan" target="_blank" rel="noopener noreferrer">keragan<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> library, I was largely inspired by <a href="https://towardsdatascience.com/generating-modern-arts-using-generative-adversarial-network-gan-on-spell-39f67f83c7b4" target="_blank" rel="noopener noreferrer">this article<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, and also by <a href="https://github.com/Maximellerbach/Car-DCGAN-Keras" target="_blank" rel="noopener noreferrer">DCGAN implementation<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> by Maxime Ellerbach, and partly by <a href="https://github.com/rkjones4/GANGogh" target="_blank" rel="noopener noreferrer">GANGogh<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> project. A lot of different GAN architectures implemented in Keras are presented <a href="https://github.com/eriklindernoren/Keras-GAN" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <h2 id="other-posts-in-azure-ml-series"><a href="#other-posts-in-azure-ml-series" class="header-anchor">#</a> Other Posts in Azure ML Series</h2> <ul><li><a href="https://soshnikov.com/azure/best-way-to-start-with-azureml/" target="_blank" rel="noopener noreferrer">Best Way to Start with Azure ML<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://soshnikov.com/azure/using-azureml-for-hyperparameter-optimization/" target="_blank" rel="noopener noreferrer">Using Azure ML for Hyperparameter Optimization<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><strong>Creating Generative Art using GANs on Azure ML</strong> (this post)</li></ul></div> <footer class="page-edit"><div class="edit-link"><a href="https://github.com/shinyzhu/azureselected/edit/master/content/cloud-advocate/2020-04/creating-generative-art-using-gan-on-azureml.md" target="_blank" rel="noopener noreferrer">Edit on GitHub</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <div class="last-updated"><span class="prefix">Last Updated:</span> <span class="time">4/14/2020, 3:59:04 AM</span></div></footer> <!----> </main></div><div class="global-ui"><!----><!----></div></div>
    <script src="/azureselected/assets/js/app.8a0b7483.js" defer></script><script src="/azureselected/assets/js/2.8d021b4d.js" defer></script><script src="/azureselected/assets/js/31.6b43ffde.js" defer></script><script src="/azureselected/assets/js/3.32c85e2e.js" defer></script><script src="/azureselected/assets/js/5.554b9b17.js" defer></script>
  </body>
</html>
